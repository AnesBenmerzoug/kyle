{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ccf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from kyle.sampling.fake_clf import DirichletFC, MultiDirichletFC\n",
    "from kyle.evaluation import EvalStats, compute_accuracy, compute_ECE, compute_expected_max\n",
    "from kyle.transformations import *\n",
    "from kyle.calibration.calibration_methods import TemperatureScaling\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78eb40",
   "metadata": {},
   "source": [
    "# Realistic Fake Classifiers\n",
    "\n",
    "It's good to have a model of what a realistic Fake Classifier should look like.\n",
    "\n",
    "Probably the simplest model for the fake classifier confidence vector distribution is the **Dirichlet Fake Classifier**:\n",
    "\n",
    "\\begin{equation}\n",
    "C \\sim Dirichlet(\\alpha_1, \\alpha_2, \\alpha_3, ...)\n",
    "\\end{equation}\n",
    "\n",
    "However, this model is possibly a bit too simple as it only has a single local maximum in the distribution. A realistic fake classifier might for example have multiple local maxima in each of the corners of the simplex, i.e. it generally is very confident in its prediction and only very rarely uncertain (center of simplex). Something similar can actually be achieved using the Dirichlet distribution by setting all the parameters $\\alpha_n < 1$. This pushes the distribution out into the corners, BUT however also out onto the sides of the simplex, which is not quite what we want. The center of a side of the simplex corresponds to a confidence vector $\\vec\\alpha = (1/\\text{num_classes}-1, 1/\\text{num_classes}-1, ..., 1/\\text{num_classes}-1, 0)$, i.e. very uncertain in all but one of the classes.\n",
    "\n",
    "Therefore we also consider two other Fake Classifiers that can have multiple local maxima in each of the corners and therefore possibly represent real neural networks better:\n",
    "\n",
    "Firstly the **Multi-Dirichlet Fake Classifier**:\n",
    "\n",
    "\\begin{align}\n",
    "K & \\sim Catgeorical(p_1, p_2, p_3, ...) \\\\\n",
    "C & \\sim Dirichlet_k(\\sigma_k\\cdot[1, 1, ..., 1, \\alpha_k, 1, ...])\n",
    "\\end{align}\n",
    "\n",
    "i.e. we first draw from a K-categorical distribution and based on the result we then draw from one of K Dirichlet distributions. Each of the K Dirichlet distributions has two parameters $\\sigma$ and $\\alpha_k$ and represent the width and position of the local maximum in the k-th corner of the simplex.\n",
    "Note: The pdf of this mixture distribution will be a weighted sum of the individual dirichlet distributions\n",
    "\n",
    "Secondly the **Multi-Gaussian Fake Classifier**:\n",
    "\n",
    "K-Categorical followed by one of K Gaussians followed by softmax\n",
    "\n",
    "(probably doesn't make too much sense as Multi-Gaussian pdf is analytically intractable due to the softmax transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05413ca4",
   "metadata": {},
   "source": [
    "In order to get an actually realistic Fake Classifier we use these three Fake Classifier models and fit their distributions to the observed confidence vector distributions for a couple of different neural networks.\n",
    "\n",
    "In this case we use:\n",
    "\n",
    "**LeNet 5** on CIFAR 10\n",
    "\n",
    "**ResNet 20** on CIFAR 10\n",
    "\n",
    "**ResNet 110** on CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606847e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cifar dataset\n",
    "#normalizarion also from https://github.com/akamaster/pytorch_resnet_cifar10\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "cifar_train_set = datasets.CIFAR10(os.getcwd(), train=True, download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
    "cifar_test_set = datasets.CIFAR10(os.getcwd(), train=False, download=True,\n",
    "                transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
    "\n",
    "cifar_train = torch.utils.data.DataLoader(cifar_train_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "cifar_test = torch.utils.data.DataLoader(cifar_test_set, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small simple LeNet5 for CIFAR 10 classification\n",
    "\n",
    "class lenet5(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, target = batch\n",
    "        output = self(x)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, target = batch\n",
    "        output = self(x)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83698ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proper implementation of ResNet18 for Cifar10. Pytorch only has ResNets for ImageNet which\n",
    "#differ in number of parameters\n",
    "#Code taken from: https://github.com/akamaster/pytorch_resnet_cifar10\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a LeNet5\n",
    "#Load selftrained LeNet 5 and pretrained Resnet20 and Resnet110 (don't have a dedicated GPU at hand D:)\n",
    "#Pretrained nets taken from https://github.com/akamaster/pytorch_resnet_cifar10\n",
    "\n",
    "#selftrained_lenet5 = lenet5()\n",
    "#checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1, save_last=True)\n",
    "#trainer = pl.Trainer(max_epochs=20, logger=False, checkpoint_callback=checkpoint_callback)\n",
    "#trainer.fit(selftrained_lenet5, cifar_train, cifar_test)\n",
    "\n",
    "selftrained_lenet5 = lenet5.load_from_checkpoint('./trained_models/lenet5.ckpt')\n",
    "\n",
    "pretrained_resnet20 = resnet20()\n",
    "pretrained_resnet110 = resnet110()\n",
    "\n",
    "pretrained_resnet20_dict = torch.load('./trained_models/resnet20-12fca82f.th',\n",
    "                               map_location=torch.device('cpu'))['state_dict']\n",
    "pretrained_resnet20_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_resnet20_dict.items()}\n",
    "pretrained_resnet20.load_state_dict(pretrained_resnet20_dict)\n",
    "\n",
    "pretrained_resnet110_dict = torch.load('./trained_models/resnet110-1d1ed7c2.th',\n",
    "                               map_location=torch.device('cpu'))['state_dict']\n",
    "pretrained_resnet110_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_resnet110_dict.items()}\n",
    "pretrained_resnet110.load_state_dict(pretrained_resnet110_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c24ab5",
   "metadata": {},
   "source": [
    "# Set which neural net to fit here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00658d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = selftrained_lenet5\n",
    "#neural_net = pretrained_resnet20\n",
    "#neural_net = pretrained_resnet110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b46a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get NN predictions on CIFAR10 test set\n",
    "\n",
    "cifar_test_full = torch.utils.data.DataLoader(cifar_test_set, batch_size=len(cifar_test_set),\n",
    "                                              shuffle=False, num_workers=2)\n",
    "images, labels = next(iter(cifar_test_full))\n",
    "\n",
    "neural_net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    logits = neural_net(images)\n",
    "    prob = F.softmax(logits, dim=1)\n",
    "    _, predicted = torch.max(prob, dim=1)\n",
    "    print(f'NLL = {F.cross_entropy(logits, labels)}')\n",
    "    print(f'accuracy = {(predicted == labels).sum().item() / labels.size(0)}')\n",
    "    \n",
    "gt_labels = labels.numpy()\n",
    "confidences = prob.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5992bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels_copy = gt_labels.copy()\n",
    "confidences_copy = confidences.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee930ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels = gt_labels_copy.copy()\n",
    "confidences = confidences_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'num non normalized confidence vectors = {np.sum((np.sum(confidences, axis=1) - 1) >= 1e-10)}')\n",
    "\n",
    "#confidences are not perfectly normalized due to floating point error\n",
    "#scipy.stats.dirichlet.pdf is very picky about normalization\n",
    "#convert confidences to float64 first for better/more accurate normalization\n",
    "\n",
    "confidences = np.array(confidences, dtype='float64')\n",
    "confidences = confidences / np.sum(confidences, axis=1)[:,None]\n",
    "print(f'num non normalized confidence vectors = {np.sum((np.sum(confidences, axis=1) - 1) >= 1e-10)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1499c9b",
   "metadata": {},
   "source": [
    "# Fitting Fake Classifiers using MLE\n",
    "\n",
    "Having gotten the confidences of our neural net on the CIFAR 10 test set we can now try and fit an appropiate fake classifier to them. This can be done quite easily for the Dirichlet and Multi-Dirichlet FC's using MLE, as we have relatively simple expressions for the distribution pdf's. The Multi-Gaussian FC is not as easy as the softmax function complicates the fake classifier's pdf. (It would be necessary to invert the softmax function, which is only possible up to an additive constant. As a result the Multi-Gaussian fake classifier's pdf will be an integral over a gaussian mixture model's pdf.)\n",
    "\n",
    "MLE is probably the easiest and simplest approach when the fake classifier's pdf is known exactly. To this end we minimize the negative log likelihood of the neural net's predicted confidence vectors under assumption of either a Dirichlet or Multi-Dirichlet distribution.\n",
    "\n",
    "As always with fitting the choice of minimization algorithm, initial guesses and bounds is important:\n",
    "\n",
    "As discussed at the start a somewhat alright fake classifier can possibly be achieved by using a simple Dirichlet Fake Classifier with alpha parameters $\\alpha_n < 1$. For fitting the DirichletFC appropiate initial guesses and bounds might therefore be $\\alpha_\\text{init} = (1,1,1,1,...)$  and $\\alpha_\\min, \\alpha_\\max = (0.0001, \\text{None})$\n",
    "\n",
    "As dicussed at the start the reasoning behind the Multi-Dirichlet FC is that each separate Dirichlet can be used to create a local maximum in one of the corners of the simplex. This only works if the full alpha vector of each dirichlet has all entries $>1$ (if any entry is $<1$ a local maximum does not exist), which means for each Dirichlet wee need $\\alpha_k >1$ and $\\sigma_k>1$. We also expect the maxima to be very 'squished' into the corners. i.e. $\\alpha_k$ to be large. For fitting the Multi-Dirichlet FC appropate initial guesses and bounds might therefore be $\\alpha_\\text{init} = (10,10,10,10,...)$ $\\sigma_\\text{init} = (2,2,2,2,...)$ (squished into corners) and $\\alpha_\\min, \\alpha_\\max = (1, \\text{None})$ $\\sigma_\\min, \\sigma_\\max = (1, \\text{None})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNet_eval = EvalStats(gt_labels, confidences)\n",
    "NNet_eval.plot_confidence_distributions([0, 1,\"top_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ad43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Dirichlet and Multi-Dirichlet FC's to the test set confidence vector distributions using MLE fitting\n",
    "\n",
    "num_classes = confidences.shape[1]\n",
    "\n",
    "Dir_FC = DirichletFC(num_classes)\n",
    "MultiDir_FC = MultiDirichletFC(num_classes)\n",
    "\n",
    "Dir_NLL = lambda parm: -np.sum(np.log( Dir_FC.pdf(confidences, parm) ))\n",
    "MultiDir_NLL = lambda parm: -np.sum(np.log( MultiDir_FC.pdf(confidences, *np.split(parm,3)) ))\n",
    "\n",
    "\n",
    "#initial guesses and bounds for fitting simple dirichlet\n",
    "init_alpha = 1*np.ones(num_classes)\n",
    "bounds_alpha = [(0.001, None)] * num_classes\n",
    "\n",
    "Dirichlet_bestfit = scipy.optimize.minimize(Dir_NLL, init_alpha, bounds=bounds_alpha, options={'disp': True})\n",
    "\n",
    "\n",
    "#initial guesses and bounds for fitting multi-dirichlet\n",
    "init_alpha = 10*np.ones(num_classes)\n",
    "init_sigma = 2*np.ones(num_classes)\n",
    "init_distribution_weights = np.ones(num_classes) / num_classes\n",
    "bounds_alpha = [(1, None)] * num_classes\n",
    "bounds_sigma = [(1, None)] * num_classes\n",
    "bounds_distribution_weights = [(0, 1)] * num_classes\n",
    "\n",
    "MultiDirichlet_bestfit = scipy.optimize.minimize(MultiDir_NLL,\n",
    "                                                 np.concatenate((init_alpha, init_sigma, init_distribution_weights)),\n",
    "                                                 bounds=bounds_alpha + bounds_sigma + bounds_distribution_weights,\n",
    "                                                 options={'disp': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set FC parameters to those found from MLE fit\n",
    "\n",
    "Dir_FC.set_alpha(Dirichlet_bestfit.x)\n",
    "MultiDir_FC.set_parameters(*np.split(MultiDirichlet_bestfit.x,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde0a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Dir_FC_eval = EvalStats(*Dir_FC.get_sample_arrays(50000))\n",
    "\n",
    "MultiDir_FC_eval = EvalStats(*MultiDir_FC.get_sample_arrays(50000))\n",
    "\n",
    "NNet_eval = EvalStats(gt_labels, confidences)\n",
    "\n",
    "Dir_FC_eval.plot_confidence_distributions([0, \"top_class\"])\n",
    "\n",
    "MultiDir_FC_eval.plot_confidence_distributions([0,\"top_class\"])\n",
    "\n",
    "NNet_eval.plot_confidence_distributions([0,\"top_class\"])\n",
    "\n",
    "#for i in range(confidences.shape[1]):\n",
    "for i in range(3):\n",
    "\n",
    "    Dir_FC_eval.plot_confidence_distributions([i])\n",
    "\n",
    "    MultiDir_FC_eval.plot_confidence_distributions([i])\n",
    "\n",
    "    NNet_eval.plot_confidence_distributions([i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad12c50",
   "metadata": {},
   "source": [
    "Above we have plotted the top class confidence distribution as well as the marginal confidence distributions for the actual neural network and our fitted fake classifiers (the full 10D confidence vector distribution is unfortuanetly a teeny bit difficult to visualise). This allows us to visually inspect how well our fake classifiers really reflect the true confidence distribution of our classifier. The 1st, 4th, ... graphs are the simple dirichlet FC. The 2nd, 5th, ... graphs are the multi-dirichlet FC. The 3rd, 6th, ... graphs are the real neural network confidence distributions.\n",
    "\n",
    "As expected the simple dirichlet FC does not work well. It doesn't capture the multimodal nature of the true marginal distribution nor the high frequency of p=1.0 confidences in the top class confidence distributions.\n",
    "\n",
    "The Multi-dirichlet FC seems to be a bit better capturing both of these qualities. However the maxima of the distributions don't lie far enough at the extremes, which is likely due to the fact that the local maximum of a dirichlet only lies exactly on the corner in the limit of $\\alpha_k \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0636c5b",
   "metadata": {},
   "source": [
    "TODO: Figure out why fitted sigma values always hit the lower bound of $\\sigma_k = 1$\n",
    "\n",
    "TODO: Split fitting of the multi dirichlet FC into fitting of singular dirichlets by assuming almost sufficient confidence and estimating distribution weights from the neural network class prediction ratios. This would turn the fitting of 30 parameters of the multi dirichlet FC into 10 fittings of 2 parameters each (only alpha and sigma, the distribution weights are estimated as stated)\n",
    "\n",
    "TODO: Try other fitting methods: e.g. moment matching or fitting only to the marginal distributions\n",
    "\n",
    "TODO: Try 'stochastic fitting' for the multi-gaussian FC where we sample the FC calculate the corrseponding marginal distributions and minimize e.g. the squared error loss between the neural network marginal distributions and the marginal distributions sampled from the FC. Could get around problem of multi-gaussian FC having analytically intractable pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc48f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Dirichlet_bestfit.x)\n",
    "print(*np.split(MultiDirichlet_bestfit.x,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed9f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
