{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Note - this cell should be executed only once per session\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# in order to get the config, it is not part of the library\n",
    "\n",
    "if os.path.basename(os.getcwd()) != \"notebooks\":\n",
    "    raise Exception(f\"Wrong directory. Did you execute this cell twice?\")\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from kyle.calibration.calibration_methods import TemperatureScaling, ClassWiseCalibration, \\\n",
    "    ConfidenceReducedCalibration, BetaCalibration, BaseCalibrationMethod, IsotonicRegression, get_binary_classification_data\n",
    "from kyle.evaluation import EvalStats\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Models and Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "\n",
    "dataset = make_classification(n_samples=60000, n_classes=n_classes, n_informative=15)\n",
    "\n",
    "X, y = dataset\n",
    "# X, y = dataset[\"data\"], dataset[\"target\"]\n",
    "\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_size = 0.5\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "\n",
    "train_index, test_index = list(sss.split(X, y))[0]\n",
    "X_train, y_train = X[train_index], y[train_index]\n",
    "X_test, y_test = X[test_index], y[test_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "confidences = model.predict_proba(X_test)\n",
    "y_pred = confidences.argmax(1)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading model and data\n",
    "\n",
    "confidences = confidences\n",
    "gt_labels = y_test\n",
    "\n",
    "\n",
    "## Visualizing Distribution of Confidences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap(\"tab10\")\n",
    "bins = 50\n",
    "\n",
    "fig, axes = plt.subplots(n_classes, figsize=(5, 5))\n",
    "fig.suptitle(\"Distribution of confidences in predicted classes\", fontsize=14)\n",
    "for count, row in enumerate(axes):\n",
    "    row.set_title(f\"Predicted Class {count}\")\n",
    "    color_left, color_right = cmap(count), cmap(count + 5)\n",
    "    max_confs = confidences[confidences.argmax(1) == count].max(1)\n",
    "    row.hist(max_confs, density=True, color=color_left, bins=bins)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Temperature Scaling in Normal, Reduced adn Class-wise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Evaluation with Train/Validation Split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_size = 0.5\n",
    "bins = 20 # for ECE\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "train_index, test_index = list(sss.split(confidences, gt_labels))[0]\n",
    "confidences_train, gt_labels_train = confidences[train_index], gt_labels[train_index]\n",
    "confidences_test, gt_labels_test = confidences[test_index], gt_labels[test_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the initial reliability curve and ECE of the resnet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uncalibrated_eval_stats = EvalStats(gt_labels_test, confidences_test, bins=bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"ECE uncalibrated: {uncalibrated_eval_stats.expected_calibration_error()}\")\n",
    "print(f\"Marginal uncalibrated: {uncalibrated_eval_stats.expected_marginal_calibration_error(1)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uncalibrated_eval_stats.plot_reliability_curves([EvalStats.TOP_CLASS_LABEL], display_weights=True)\n",
    "plt.title(\"Uncalibrated reliabilities\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reduced Temp Scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_scaling_full = TemperatureScaling()\n",
    "t_scaling_binary = ConfidenceReducedCalibration()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_scaling_full.fit(confidences_train, gt_labels_train)\n",
    "t_scaling_binary.fit(confidences_train, gt_labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recalibrated_full_confs = t_scaling_full.get_calibrated_confidences(confidences_test)\n",
    "recalibrated_reduced_confs = t_scaling_binary.get_calibrated_confidences(confidences_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recalibrated_full_eval_stats = EvalStats(gt_labels_test, recalibrated_full_confs, bins=bins)\n",
    "recalibrated_binary_eval_stats = EvalStats(gt_labels_test, recalibrated_reduced_confs, bins=bins)\n",
    "\n",
    "print(f\"Temp Scaling ECE: {recalibrated_full_eval_stats.expected_calibration_error()}\")\n",
    "print(f\"Reduced Temp Scaling ECE: {recalibrated_binary_eval_stats.expected_calibration_error()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bin_confs, bin_gt  = get_binary_classification_data(recalibrated_reduced_confs, gt_labels_test)\n",
    "bin_eval_stats = EvalStats(bin_gt, bin_confs, bins=bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bin_eval_stats.plot_reliability_curves([0])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recalibrated_full_eval_stats.plot_reliability_curves([EvalStats.TOP_CLASS_LABEL], display_weights=True)\n",
    "plt.title(\"Temp scaling\")\n",
    "plt.show()\n",
    "\n",
    "recalibrated_binary_eval_stats.plot_reliability_curves([EvalStats.TOP_CLASS_LABEL], display_weights=True)\n",
    "plt.title(\"Reduced temp scaling\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class-wise Temp Scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "classwise_scaler = ClassWiseCalibration()\n",
    "classwise_scaler.fit(confidences_train, gt_labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classwise_recalibrated_confs = classwise_scaler.get_calibrated_confidences(confidences_test)\n",
    "classwise_eval_stats = EvalStats(gt_labels_test, classwise_recalibrated_confs, bins=bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classwise_eval_stats.plot_reliability_curves([EvalStats.TOP_CLASS_LABEL], display_weights=True)\n",
    "plt.title(\"Class-wise Calibrated\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Class-wise Temp Scaling ECE: {classwise_eval_stats.expected_calibration_error()}\")\n",
    "print(f\"Temp Scaling ECE: {recalibrated_full_eval_stats.expected_calibration_error()}\")\n",
    "\n",
    "print(f\"Class-wise Temp Scaling cwECE: {classwise_eval_stats.class_wise_expected_calibration_error()}\")\n",
    "print(f\"Temp Scaling cwECE: {recalibrated_full_eval_stats.class_wise_expected_calibration_error()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEFAULT_WRAPPERS = {\n",
    "    \"Baseline\": lambda method_factory: method_factory(),\n",
    "    \"Class-wise\": lambda method_factory: ClassWiseCalibration(method_factory),\n",
    "    \"Reduced\": lambda method_factory: ConfidenceReducedCalibration(method_factory()),\n",
    "    \"Class-wise reduced\": lambda method_factory:\n",
    "                            ClassWiseCalibration(lambda : ConfidenceReducedCalibration(method_factory())),\n",
    "}\n",
    "\n",
    "DEFAULT_CV = 5\n",
    "DEFAULT_BINS = 20\n",
    "\n",
    "ALL_CALIBRATION_METHOD_FACTORIES = (TemperatureScaling, BetaCalibration, IsotonicRegression)\n",
    "ALL_METRICS = (\"ECE\", \"cwECE\")\n",
    "\n",
    "def compute_score(scaler, confs: np.ndarray, labels: np.ndarray, bins, metric=\"ECE\"):\n",
    "    calibrated_confs = scaler.get_calibrated_confidences(confs)\n",
    "    eval_stats = EvalStats(labels, calibrated_confs, bins=bins)\n",
    "    if metric == \"ECE\":\n",
    "        return eval_stats.expected_calibration_error()\n",
    "    elif metric == \"cwECE\":\n",
    "        return eval_stats.class_wise_expected_calibration_error()\n",
    "    elif isinstance(metric, int):\n",
    "        return eval_stats.expected_marginal_calibration_error(metric)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric {metric}\")\n",
    "\n",
    "def get_scores(scaler, metric, cv, bins):\n",
    "    scoring = lambda *args: compute_score(*args, bins=bins, metric=metric)\n",
    "    return cross_val_score(scaler, confidences, gt_labels, scoring=scoring, cv=cv)\n",
    "\n",
    "def plot_scores(labels, scores_collection, title=\"\", ax=None, y_lim=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(14,7))\n",
    "        ax = fig.axes\n",
    "    ax.set_title(title)\n",
    "    ax.boxplot(scores_collection, labels=labels)\n",
    "    if y_lim is not None:\n",
    "        ax.set_ylim(y_lim)\n",
    "    # Does not work this way, has to be set somewhere else. Why do we need this?\n",
    "    # ax.set_xticks(rotation=70)\n",
    "\n",
    "\n",
    "def evaluate_calibration_wrappers(method_factory, wrappers_dict=None, metric=\"ECE\", cv=DEFAULT_CV, method_name=None,\n",
    "        bins=DEFAULT_BINS, ax=None, short_title=False, y_lim=None):\n",
    "    if method_name is None:\n",
    "        method_name = method_factory.__name__\n",
    "    if short_title:\n",
    "        plot_title = f\"{method_name}\"\n",
    "    else:\n",
    "        plot_title = f\"Evaluating wrappers of {method_name} on metric {metric} with {bins} bins\\n \" \\\n",
    "                     f\"CV with {cv} folds on {len(confidences)} data points.\"\n",
    "    if wrappers_dict is None:\n",
    "        wrappers_dict = DEFAULT_WRAPPERS\n",
    "\n",
    "    labels = []\n",
    "    scores_collection = []\n",
    "    for label, wrapper in wrappers_dict.items():\n",
    "        labels.append(label)\n",
    "        method = wrapper(method_factory)\n",
    "        scores = get_scores(method, metric, cv=cv, bins=bins)\n",
    "        scores_collection.append(scores)\n",
    "    plot_scores(labels, scores_collection, title=plot_title, ax=ax, y_lim=y_lim)\n",
    "    return labels, scores_collection # just in case we wanna do more than plotting\n",
    "\n",
    "Y_LIM = (0.001, 0.22) # taken such that minimum and maximum are visible in all plots\n",
    "\n",
    "def perform_default_evaluation(method_factories=ALL_CALIBRATION_METHOD_FACTORIES, metrics=ALL_METRICS,\n",
    "       figsize=(20, 7), y_lim=Y_LIM):\n",
    "    \"\"\"This may take a while with all methods and metrics\"\"\"\n",
    "    for metric in metrics:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=len(method_factories), figsize=figsize)\n",
    "        if len(method_factories) == 1: # axes fails to be a list if ncols=1\n",
    "            axes = [axes]\n",
    "        for col, method_factory in zip(axes, method_factories):\n",
    "            evaluate_calibration_wrappers(method_factory, metric=metric, ax=col, short_title=True, y_lim=y_lim)\n",
    "        fig.suptitle(f\"Evaluation with {metric} ({DEFAULT_CV} folds; {DEFAULT_BINS} bins)\")\n",
    "        fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perform_default_evaluation()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}