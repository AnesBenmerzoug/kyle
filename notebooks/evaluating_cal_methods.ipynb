{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Note - this cell should be executed only once per session\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# in order to get the config, it is not part of the library\n",
    "\n",
    "if os.path.basename(os.getcwd()) != \"notebooks\":\n",
    "    raise Exception(f\"Wrong directory. Did you execute this cell twice?\")\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kyle.calibration.calibration_methods import *\n",
    "from kyle.evaluation import EvalStats\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_calibration_dataset(n_classes=5, weights=None, n_samples=30000, n_informative=15,  model=RandomForestClassifier()):\n",
    "    n_dataset_samples = 2 * n_samples\n",
    "    test_size = 0.5\n",
    "    X, y = make_classification(n_samples=n_dataset_samples, n_classes=n_classes,\n",
    "                                  n_informative=n_informative, weights=weights)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "\n",
    "    train_index, test_index = list(sss.split(X, y))[0]\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test = X[test_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    confidences = model.predict_proba(X_test)\n",
    "    y_pred = confidences.argmax(1)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print(f\"Model accuracy: {accuracy}\")\n",
    "    return confidences, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEFAULT_WRAPPERS = {\n",
    "    \"Baseline\": lambda method_factory: method_factory(),\n",
    "    \"Class-wise\": lambda method_factory: ClassWiseCalibration(method_factory),\n",
    "    \"Reduced\": lambda method_factory: ConfidenceReducedCalibration(method_factory()),\n",
    "    \"Class-wise reduced\": lambda method_factory:\n",
    "                            ClassWiseCalibration(lambda : ConfidenceReducedCalibration(method_factory())),\n",
    "}\n",
    "\n",
    "DEFAULT_CV = 4\n",
    "DEFAULT_BINS = 20\n",
    "\n",
    "ALL_CALIBRATION_METHOD_FACTORIES = (\n",
    "    # TemperatureScaling,\n",
    "    BetaCalibration,\n",
    "    # LogisticCalibration,\n",
    "    IsotonicRegression,\n",
    "    HistogramBinning,\n",
    ")\n",
    "ALL_METRICS = (\n",
    "    \"ECE\",\n",
    "    \"cwECE\",\n",
    ")\n",
    "\n",
    "\n",
    "def compute_score(scaler, confs: np.ndarray, labels: np.ndarray, bins, metric=\"ECE\"):\n",
    "    calibrated_confs = scaler.get_calibrated_confidences(confs)\n",
    "    eval_stats = EvalStats(labels, calibrated_confs, bins=bins)\n",
    "    if metric == \"ECE\":\n",
    "        return eval_stats.expected_calibration_error()\n",
    "    elif metric == \"cwECE\":\n",
    "        return eval_stats.class_wise_expected_calibration_error()\n",
    "    elif isinstance(metric, int):\n",
    "        return eval_stats.expected_marginal_calibration_error(metric)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric {metric}\")\n",
    "\n",
    "def get_scores(scaler, metric, cv, bins, confs, labels):\n",
    "    scoring = lambda *args: compute_score(*args, bins=bins, metric=metric)\n",
    "    return cross_val_score(scaler, confs, labels, scoring=scoring, cv=cv)\n",
    "\n",
    "def plot_scores(wrapper_scores_dict: dict, title=\"\", ax=None, y_lim=None):\n",
    "    labels = wrapper_scores_dict.keys()\n",
    "    scores_collection = wrapper_scores_dict.values()\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(14,7))\n",
    "        ax = plt.gca()\n",
    "    ax.set_title(title)\n",
    "    ax.boxplot(scores_collection, labels=labels)\n",
    "    if y_lim is not None:\n",
    "        ax.set_ylim(y_lim)\n",
    "\n",
    "def evaluate_calibration_wrappers(method_factory, confidences, gt_labels, wrappers_dict=None, metric=\"ECE\",\n",
    "        cv=DEFAULT_CV, method_name=None, bins=DEFAULT_BINS, short_description=False):\n",
    "    if method_name is None:\n",
    "        method_name = method_factory.__name__\n",
    "    if short_description:\n",
    "        description = f\"{method_name}\"\n",
    "    else:\n",
    "        description = f\"Evaluating wrappers of {method_name} on metric {metric} with {bins} bins\\n \" \\\n",
    "                     f\"CV with {cv} folds on {len(confidences)} data points.\"\n",
    "    if wrappers_dict is None:\n",
    "        wrappers_dict = DEFAULT_WRAPPERS\n",
    "\n",
    "    wrapper_scores_dict = {}\n",
    "    for wrapper_name, wrapper in wrappers_dict.items():\n",
    "        method = wrapper(method_factory)\n",
    "        scores = get_scores(method, metric, cv=cv, bins=bins, confs=confidences, labels=gt_labels)\n",
    "        wrapper_scores_dict[wrapper_name] = scores\n",
    "    return wrapper_scores_dict, description\n",
    "\n",
    "# taken such that minimum and maximum are visible in all plots\n",
    "DEFAULT_Y_LIMS_DICT = {\n",
    "    \"ECE\": (0.004, 0.03),\n",
    "    \"cwECE\": (0.009, 0.01),\n",
    "}\n",
    "\n",
    "def perform_default_evaluation(confidences, gt_labels, method_factories=ALL_CALIBRATION_METHOD_FACTORIES, metrics=ALL_METRICS):\n",
    "    evaluation_results = defaultdict(list)\n",
    "    for metric in metrics:\n",
    "        print(f\"Creating evaluation for {metric}\")\n",
    "        for method_factory in method_factories:\n",
    "            print(f\"Computing scores for {method_factory.__name__}\", end=\"\\r\")\n",
    "            result = evaluate_calibration_wrappers(method_factory, confidences=confidences, gt_labels=gt_labels,\n",
    "                                              metric=metric, short_description=True)\n",
    "            evaluation_results[metric].append(result)\n",
    "    return evaluation_results\n",
    "\n",
    "def plot_default_evaluation_results(evaluation_results: dict, figsize=(25, 7), y_lims_dict=None, title_addon=None):\n",
    "    if y_lims_dict is None:\n",
    "        y_lims_dict = DEFAULT_Y_LIMS_DICT\n",
    "    ncols = len(list(evaluation_results.values())[0])\n",
    "    for metric, results in evaluation_results.items():\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=ncols, figsize=figsize)\n",
    "        y_lim = y_lims_dict[metric]\n",
    "        if ncols == 1: # axes fails to be a list if ncols=1\n",
    "            axes = [axes]\n",
    "        for col, result in zip(axes, results):\n",
    "            wrapper_scores_dict, description = result\n",
    "            plot_scores(wrapper_scores_dict, title=description, ax=col, y_lim=y_lim)\n",
    "\n",
    "        title = f\"Evaluation with {metric} ({DEFAULT_CV} folds; {DEFAULT_BINS} bins)\"\n",
    "        if title_addon is not None:\n",
    "            title += f\"\\n{title_addon}\"\n",
    "        fig.suptitle(title)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "balanced_confs, balanced_gt = get_calibration_dataset()\n",
    "unbalanced_confs, unbalanced_gt = get_calibration_dataset(weights=(0.3, 0.1, 0.25, 0.15))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluating wrappers on a single calibration method\n",
    "\n",
    "balanced_scores_ECE, description = evaluate_calibration_wrappers(BetaCalibration, confidences=balanced_confs,\n",
    "                                                    gt_labels=balanced_gt, metric=\"ECE\")\n",
    "\n",
    "plot_scores(balanced_scores_ECE, title=description)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unbalanced_scores_ECE, description = evaluate_calibration_wrappers(BetaCalibration, confidences=unbalanced_confs,\n",
    "                                                    gt_labels=unbalanced_gt, metric=\"ECE\")\n",
    "\n",
    "plot_scores(unbalanced_scores_ECE, title=description)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating wrappers on multiple metrics and plotting next to each other"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_results = perform_default_evaluation(confidences=balanced_confs, gt_labels=balanced_gt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_default_evaluation_results(eval_results, title_addon=\"Balanced\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}