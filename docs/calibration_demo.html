

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>What is calibration? &mdash; Python 0.1.2.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Dirichlet fake classifiers" href="fake_classifiers.html" />
    <link rel="prev" title="kyle library and game" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Python
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guides and Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">What is calibration?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Model-calibration">Model calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Model-agnostic-calibration">Model-agnostic calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="fake_classifiers.html">Dirichlet fake classifiers</a></li>
<li class="toctree-l1"><a class="reference internal" href="fake_classifiers.html#Analytical-results">Analytical results</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting started</a></li>
</ul>
<p class="caption"><span class="caption-text">Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="kyle/index.html">Library Modules</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Python</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>What is calibration?</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/calibration_demo.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="kn">from</span> <span class="nn">kyle.calibration</span> <span class="kn">import</span> <span class="n">ModelCalibrator</span>
<span class="kn">from</span> <span class="nn">kyle.models</span> <span class="kn">import</span> <span class="n">CalibratableModel</span>
<span class="kn">from</span> <span class="nn">kyle.metrics</span> <span class="kn">import</span> <span class="n">ECE</span>
<span class="kn">from</span> <span class="nn">kyle.calibration.calibration_methods</span> <span class="kn">import</span> <span class="n">TemperatureScaling</span>
<span class="kn">from</span> <span class="nn">kyle.sampling.fake_clf</span> <span class="kn">import</span> <span class="n">DirichletFC</span>
<span class="kn">from</span> <span class="nn">kyle.transformations</span> <span class="kn">import</span> <span class="n">MaxComponentSimplexAut</span>
<span class="kn">from</span> <span class="nn">kyle.evaluation</span> <span class="kn">import</span> <span class="n">EvalStats</span>
</pre></div>
</div>
</div>
<div class="section" id="What-is-calibration?">
<h1>What is calibration?<a class="headerlink" href="#What-is-calibration?" title="Permalink to this headline">¶</a></h1>
<p>When we talk about how good a machine learning model is, what we (generally) mean to ask is: How accurate is the model? While this is a good enough metric in many cases, we are, in fact, leaving out important information about the model. One such piece of information is concerned with whether the confidence of the model is in line with its accuracy. If it is, we say the model is calibrated.</p>
<p>To explain this concept in detail, let’s begin with an example. Suppose we want to predict whether a patient has cancer. We can simulate data with two classes i.e. <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span> where <span class="math notranslate nohighlight">\(y=0\)</span> denotes a healthy patient and <span class="math notranslate nohighlight">\(y=1\)</span> denotes a patient who has cancer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span>
        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
        <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">n_informative</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">n_redundant</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="n">n_classes</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can then train a neural network on our data:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/runner/work/kyle/kyle/.tox/py/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
MLPClassifier(hidden_layer_sizes=(50, 50, 50))
</pre></div></div>
</div>
<p>and make predictions on new samples. Let’s see how our model performs on unseen examples:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">model_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="sa">f</span><span class="s2">&quot;Model accuracy: </span><span class="si">{</span><span class="n">model_accuracy</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">%&quot;</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;Model accuracy: 93.5%&#39;
</pre></div></div>
</div>
<p>That seems pretty good! One might think our job here is done: After all, the model predicts whether a person has cancer or not with decent accuracy. Unfortunately, accuracy of a model does not tell us the full story. This is so due to the fact that at inference time, for a given sample a model outputs confidence scores for each class. We then take the class with the highest confidence and interpret that as the prediction of the model.</p>
<p>This conversion of continuous (probability) to discrete (label) values can hide certain properties of the model. To illustrate this, let’s take two models – <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> – trained on the same data. Let’s further assume they have similar accuracy. Suppose we test both models with 10 healthy samples. <span class="math notranslate nohighlight">\(A\)</span> assigns probabilities <span class="math notranslate nohighlight">\((0.49, 0.51)\)</span> to all samples, whereas <span class="math notranslate nohighlight">\(B\)</span> assigns <span class="math notranslate nohighlight">\((0.1, 0.9)\)</span>. While <span class="math notranslate nohighlight">\(A\)</span> &amp; <span class="math notranslate nohighlight">\(B\)</span> will be wrong 100% of the time, notice
<span class="math notranslate nohighlight">\(A\)</span> being much closer to classifying the samples as belonging to the correct class compared to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>Continuing with our previous example: Imagine that on all examples where the model was <span class="math notranslate nohighlight">\(95\)</span>% confident that the subject has cancer, it was correct <span class="math notranslate nohighlight">\(70\)</span>% of the time. Intuitively, it seems there’s something not quite right with the model: the model is over-confident in its predictions. This notion is formalized by the concept of calibration. We say a model is (strongly) calibrated when, for any confidence value <span class="math notranslate nohighlight">\(p \in [0, 1]\)</span>, prediction of a class with confidence <span class="math notranslate nohighlight">\(p\)</span>
is correct with probability <span class="math notranslate nohighlight">\(p\)</span>:</p>
<p><span class="math">\begin{equation}
P(\widehat{y}=y|\widehat{p}=p) = p \quad \forall p \in [0, 1]
\end{equation}</span></p>
<p>So, is our model calibrated? As we can see in the equation above, <span class="math notranslate nohighlight">\(\widehat{p}\)</span> is continuous, which means we cannot compute the equation with finite data. We can, however, develop empirical measures that approximate the true measure of (mis)calibration.</p>
<p>One simple way to get an empirical estimate of the model’s accuracy and confidence is to discretize the probability space. This is done by slicing <span class="math notranslate nohighlight">\(p\)</span> into <span class="math notranslate nohighlight">\(K\)</span> equal-sized bins. We can then calculate the accuracy and confidence for each bin:</p>
<p><span class="math">\begin{equation}
accuracy_{B_k} = \frac{1}{|B_k|} \sum_{m=1}^{|B_k|}1(\widehat{p}_m=p_m)
\end{equation}</span></p>
<p><span class="math">\begin{equation}
confidence_{B_k} = \frac{1}{|B_k|} \sum_{m=1}^{|B_k|}\widehat{p}_m
\end{equation}</span></p>
<p>We can now simply calculate the weighted average difference between the accuracy and confidence of the model over all bins:</p>
<p><span class="math">\begin{equation}
\sum_{k=1}^{K} \frac{|B_k|}{n} \Big|\:accuracy_{B_k} - confidence_{B_k} \Big|
\end{equation}</span></p>
<p>This is known as the <strong>Expected Calibration Error</strong> <span class="math notranslate nohighlight">\((ECE).\)</span> As can be seen, <span class="math notranslate nohighlight">\(ECE=0\)</span> if a model is perfectly calibrated. Let’s calculate the <span class="math notranslate nohighlight">\(ECE\)</span> for our model with <span class="math notranslate nohighlight">\(10\)</span> bins:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ece</span> <span class="o">=</span> <span class="n">ECE</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Evaluate uncalibrated predictions</span>
<span class="n">uncalibrated_confidences</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">pre_calibration_ece</span> <span class="o">=</span> <span class="n">ece</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">uncalibrated_confidences</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="sa">f</span><span class="s2">&quot;ECE before calibration: </span><span class="si">{</span><span class="n">pre_calibration_ece</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;ECE before calibration: 0.0669516487805154&#39;
</pre></div></div>
</div>
<p>We can also visualize the extent of miscalibration by plotting the model’s confidence <em>(x-axis)</em> vs. the ground truth probability <em>(y-axis)</em>. For a perfectly calibrated model, the plot should be <span class="math notranslate nohighlight">\(y=x\)</span>. Let’s see how our model fares:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">eval_stats</span> <span class="o">=</span> <span class="n">EvalStats</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">uncalibrated_confidences</span><span class="p">)</span>
<span class="n">class_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)]</span>

<span class="n">eval_stats</span><span class="o">.</span><span class="n">plot_reliability_curves</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/calibration_demo_16_0.png" src="_images/calibration_demo_16_0.png" />
</div>
</div>
<p>Okay, so our model is not calibrated as <span class="math notranslate nohighlight">\(ECE&gt;0\)</span>. Can we do anything to remedy the situation?</p>
</div>
<div class="section" id="Model-calibration">
<h1>Model calibration<a class="headerlink" href="#Model-calibration" title="Permalink to this headline">¶</a></h1>
<p>Indeed, we can improve the calibration of our model using various techniques. What’s more, we don’t need to train our model again; many calibration techniques are post-processing methods i.e. operating on the trained model’s output confidence scores. The output scores for calibration are typically obtained on a validation set.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">kyle</span></code>, we have provided a <code class="docutils literal notranslate"><span class="pre">CalibratableModel</span></code> class which takes a model and, as the name suggests, makes it possible to calibrate that model. By default, we use a technique called <a class="reference external" href="https://arxiv.org/abs/1706.04599">Temperature scaling</a> for calibration.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create calibratable model</span>
<span class="n">calibration_method</span> <span class="o">=</span> <span class="n">TemperatureScaling</span><span class="p">()</span>
<span class="n">calibratable_model</span> <span class="o">=</span> <span class="n">CalibratableModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">calibration_method</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We also provide a <code class="docutils literal notranslate"><span class="pre">ModelCalibrator</span></code> class which holds the data to calibrate models:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create model calibrator and calibrate model</span>
<span class="n">calibrator</span> <span class="o">=</span> <span class="n">ModelCalibrator</span><span class="p">(</span><span class="n">X_calibrate</span><span class="o">=</span><span class="n">X_test</span><span class="p">,</span>
                             <span class="n">y_calibrate</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
                             <span class="n">X_fit</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
                             <span class="n">y_fit</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We now have everything ready to calibrate our model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">calibrator</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">calibratable_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s see if calibrating the model improved the <span class="math notranslate nohighlight">\(ECE\)</span> score</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Passing X_test instead of X_calibrate in predict_proba() to make comparison with pre-calib model clear,</span>
<span class="c1"># same reasong for y_test in ece.compute()</span>
<span class="n">calibrated_confidences</span> <span class="o">=</span> <span class="n">calibratable_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">post_calibration_ece</span> <span class="o">=</span> <span class="n">ece</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">calibrated_confidences</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="sa">f</span><span class="s2">&quot;ECE before calibration: </span><span class="si">{</span><span class="n">pre_calibration_ece</span><span class="si">}</span><span class="s2">, ECE after calibration: </span><span class="si">{</span><span class="n">post_calibration_ece</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;ECE before calibration: 0.0669516487805154, ECE after calibration: 0.022424558162753988&#39;
</pre></div></div>
</div>
<p>Great! <span class="math notranslate nohighlight">\(ECE\)</span> has improved. Let’s also plot a reliability curve to visually confirm the improvement in calibration.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">eval_stats</span> <span class="o">=</span> <span class="n">EvalStats</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">calibrated_confidences</span><span class="p">)</span>

<span class="n">eval_stats</span><span class="o">.</span><span class="n">plot_reliability_curves</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/calibration_demo_28_0.png" src="_images/calibration_demo_28_0.png" />
</div>
</div>
<p>Wonderful! We have successfully improved our model’s calibration.</p>
</div>
<div class="section" id="Model-agnostic-calibration">
<h1>Model-agnostic calibration<a class="headerlink" href="#Model-agnostic-calibration" title="Permalink to this headline">¶</a></h1>
<p>You may have noticed that to evaluate (mis)calibration of a model, we don’t require the model itself. Rather, it is sufficient to have the confidence scores predicted by the model. This means we can abstract away the model and generate both the ground truth and confidence scores via sampling processes.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">kyle</span></code> we have provided samplers that simulate different kinds of calibration properties. One such sampler is the <code class="docutils literal notranslate"><span class="pre">DirichletFC</span></code> class which provides calibrated ground truth and confidences by default.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sampler</span> <span class="o">=</span> <span class="n">DirichletFC</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Get 1000 calibrated fake confidence scores</span>
<span class="n">calibrated_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_sample_arrays</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">ground_truth</span><span class="p">,</span> <span class="n">confidences</span> <span class="o">=</span> <span class="n">calibrated_samples</span>
</pre></div>
</div>
</div>
<p>Let’s evaluate the <span class="math notranslate nohighlight">\(ECE\)</span> for these samples:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ece</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.02273518399897774
</pre></div></div>
</div>
<p>Wait, the <span class="math notranslate nohighlight">\(ECE&gt;0\)</span>, how can we say that the samples are calibrated?</p>
<p>As mentioned earlier, we only have finite samples so true miscalibration can only be measured asymptotically. This means that the more samples we have, the more accurate would <span class="math notranslate nohighlight">\(ECE\)</span>’s estimate become. We can test this by generating <em>5x</em> as many samples as before and evaluating <span class="math notranslate nohighlight">\(ECE\)</span> again:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">calibrated_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_sample_arrays</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">ground_truth</span><span class="p">,</span> <span class="n">confidences</span> <span class="o">=</span> <span class="n">calibrated_samples</span>

<span class="n">ece</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.00955290612159993
</pre></div></div>
</div>
<p>As expected, <span class="math notranslate nohighlight">\(ECE\)</span> goes down with more samples.</p>
<p>We can also systematically generate uncalibrated samples. For instance, the <code class="docutils literal notranslate"><span class="pre">ShiftingSimplexAutomorphism</span></code> shifts the confidence scores by adding a fixed vector with positive entries to the input and normalizing the result.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">overestimating_max</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">automorphism</span> <span class="o">=</span> <span class="n">MaxComponentSimplexAut</span><span class="p">(</span><span class="n">overestimating_max</span><span class="p">)</span>
<span class="n">shifted_sampler</span> <span class="o">=</span> <span class="n">DirichletFC</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">simplex_automorphism</span><span class="o">=</span><span class="n">automorphism</span><span class="p">)</span>

<span class="c1"># Get 1000 uncalibrated fake confidence scores</span>
<span class="n">uncalibrated_samples</span> <span class="o">=</span> <span class="n">shifted_sampler</span><span class="o">.</span><span class="n">get_sample_arrays</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">ground_truth</span><span class="p">,</span> <span class="n">confidences</span> <span class="o">=</span> <span class="n">uncalibrated_samples</span>
</pre></div>
</div>
</div>
<p>Let’s see if the uncalibrated nature of the samples is validated by <span class="math notranslate nohighlight">\(ECE\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ece</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.17369679110861524
</pre></div></div>
</div>
<p>Once again, to verify that miscalibration will indeed increase with more samples, let’s sample <em>5x</em> as many samples as before and measure <span class="math notranslate nohighlight">\(ECE\)</span> again:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">uncalibrated_samples</span> <span class="o">=</span> <span class="n">shifted_sampler</span><span class="o">.</span><span class="n">get_sample_arrays</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">ground_truth</span><span class="p">,</span> <span class="n">confidences</span> <span class="o">=</span> <span class="n">uncalibrated_samples</span>

<span class="n">ece</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.18603759557547142
</pre></div></div>
</div>
<p>Great! Calibration error goes up as we sample more instances.</p>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="fake_classifiers.html" class="btn btn-neutral float-right" title="Dirichlet fake classifiers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="kyle library and game" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright .

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>